---
title: "MovieLens report"
author: "Nina Caparros"
date: "17/10/2019"
output: pdf_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Initialisation, include=FALSE}
#------------------------------------------------------------------------
#This part of the code was provided in the assessment, by HarvardX

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

#Some additionnal packages
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(ggpubr)) install.packages("ggpubr", repos = "http://cran.us.r-project.org")

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data

set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```

```{r Counts, include=FALSE}
#Number of rows and columns of the dataset provided by the edx dataset
nrows <- nrow(edx)
ncols <-ncol(edx)

#Number of distinct movies and users
number_of_movies <- edx %>% group_by(movieId) %>% summarize() %>% nrow()
number_of_users <- edx %>% group_by(userId) %>% summarize() %>% nrow()

```

## Introduction

The following report is the analysis and results of the MovieLens Assessment of the Data Science Program of HarvardX, available on Edx (https://https://courses.edx.org/courses/course-v1:HarvardX+PH125.9x+2T2019/course/). The purpose of this project was to develop a movie recommendation system using a subset of the MovieLens dataset. MovieLenses datasets are available on grouplens.org and several sizes are at one's disposal. As required in the assessment, one was using the MovieLens 10M Dataset, which provides roughly ten millions (`r nrows`) of movie ratings. The initialization of the dataset was provided at the begining of the assessment by HarvardX. Each row of the dataset represented a rating, of one movie, by one user, at a certain time.

## Overview

The dataset initialized was made of six columns as follow :

* userId : the identifier of the user relative of the rating in the row
* movieId : the identifier of the movie rated in the row
* rating : the rating given by the user, which can take values from 0 to 5, as whole star ratings (0 to 5) and half star ratings (0.5 to 4.5)
* timestamp : the date and time as a timestamp at which the user left it's rating
* title : the title and year of release of the movie rated
* genres : the genre or genres of the movie rated

| Parameter name 	| Class	| Number of distinct values | Minimum value 	|  Maximum value 	|  Mean value 	| Median value |
|:----:	|:-:	|:-:	|:-:	|:-:	|
| userId 	|  `r class(edx$userId)` 	| `r number_of_users` |  Not relevant 	|  Not relevant 	| Not relevant | Not relevant | 
| movieId	| `r class(edx$movieId)` | `r number_of_movies` |Not relevant 	|  Not relevant 	| Not relevant | Not relevant | 
| rating	| `r class(edx$rating)`	| `r nrow(edx %>% filter(!is.na(rating)))` |  `r min(edx$rating)` 	| `r max(edx$rating)`	| `r mean(edx$rating)` | `r median(edx$rating)` |
| timestamp 	|  `r class(edx$timestamp)` 	| `r nrow(edx %>% filter(!is.na(timestamp)))` |  `r min(edx$timestamp)` 	| `r max(edx$timestamp)`	| `r mean(edx$timestamp)` | `r median(edx$timestamp)` |
| title	| `r class(edx$title)` | `r number_of_movies` |Not relevant 	|  Not relevant 	| Not relevant | Not relevant | 
| genres	| `r class(edx$genres)`	| `r length(edx %>% pull(genres)%>% unique())`  |Not relevant 	|  Not relevant 	| Not relevant | Not relevant | 

A first glance at the current dataset showed that : 

* The timestamp information clearly appeared uninterpretable. 
* The information of the year of release was contained in the title column.

Before further investigation, one needed to clean those timestamp values and extract the release's year.

### Data cleaning

The first step one took was converting the `timestamp` column into a `date` (Year-Month-Day) and `time` (Hour:Minute) columns. The original `timestamp` column was removed. This step allowed one to analyse datas by date and hour of the day. The validation dataset had been cleaned in the same way.

One wondered if the time of the day, the day of the month, the month of the year, or even the year were influencing the ratings of the users. Could the date influence the rating of one given user ? Could the rating of that particular user change depending of the period ?

It seemed also appropriate to extract the details of the date (as year, month and day) for further analysis.

```{r Converting dates, include=FALSE}
edx <- edx %>% 
  mutate(
    date = format(as_datetime(timestamp), format="%Y-%m-%d"), 
    time=format(as_datetime(timestamp), format="%H:%M"),
    year=year(date),
    month=month(date),
    day=day(date)) %>% 
  select(-timestamp)


validation <- validation %>% 
  mutate(
    date = format(as_datetime(timestamp), format="%Y-%m-%d"), 
    time=format(as_datetime(timestamp), format="%H:%M"),
     year=year(date),
    month=month(date),
    day=day(date)) %>% 
  select(-timestamp)

```


The next step was to extract the year the movie was released and add it to both the `edx` and `validation` datasets.
In order to do this, one had to process the `title` column, which contained the title and the year of release between parenthesis. The `title would` contain only the title, and the new column `yearOfRelease` would contain the year previously stored in the title. 

As some titles had some string characters between parenthesis, and since the structure of the title (`title (year of release)`) was always the same, one decided to simply use `substring` instead of a `regex`.

```{r Extracting year of release, include=FALSE}
edx <- edx %>% mutate(
  yearOfRelease = as.numeric(substring(title, nchar(title)-4,nchar(title)-1)),
  title = substring(title, 1,nchar(title)-7))

validation <- validation %>% mutate(
    yearOfRelease = as.numeric(substring(title, nchar(title)-4,nchar(title)-1)),
    title = substring(title, 1,nchar(title)-7))
```

The cleaned dataset looked like : 

```{r Preview of converted dates, include=FALSE}
head(edx)
```

### Movies

The `edx` dataset provided ratings for `number_of_movies` different movies.

```{r Movies, include=FALSE}
# Display whole integers, not scientific notation
options("scipen"=100)

# Movie ratings analysis : top 10 reviewed movies
dataByMovie <- edx %>% 
  group_by(movieId) %>% 
  summarize(n=n()) %>%
  top_n(10,n)

# Add total ratings count to the top 10 reviewed movies
completeDataByMovie <- left_join(edx %>%
                                   filter(movieId %in% dataByMovie$movieId),
                                 dataByMovie,
                                 by="movieId")

#Initialize the plot
plotByMovie <- ggplot()

#Bar chart of the 10 most rated movies (x) versus the number of ratings for each movie (y)
ratingByMovieBarPlot <- plotByMovie  + 
  geom_bar(data= dataByMovie %>%
             group_by(movieId),
           aes(x=reorder(title,-n),
               y=n),
           stat="identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  ylab("Number of ratings") + 
  xlab("Movies (top 10 rated)") + 
  coord_cartesian(xlim =c(1, 10))

#Boxplot of the ratings for the 10 most rated movies
MovieAvgRatingBoxplot <- plotByMovie + 
  geom_boxplot(
    data=completeDataByMovie
    ,aes(
      x=reorder(title,-n),
      y=rating)
    ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  ylab("Ratings") + 
  xlab("Movies (top 10 rated)") + 
  coord_cartesian(xlim =c(1, 10))

#Ploting the barchart and the boxplot side by side
ggarrange(ratingByMovieBarPlot,MovieAvgRatingBoxplot)

```

It appeared that the most rated movies were not necessarily the best rated movies, and the distribution of the ratings varied substantially. For instance, in the right plot, the first most rated movie (Pulp Fiction) has 50% of it's ratings between 4 and 5, with a median of 4.5, while Jurassic Park, the fourth most rated movie has 50% of it's ratings between 3 and 4, and 25% of it's ratings equal to 4. The Shawshank Redemption (fifth most rated movie) even has 25% of it's ratings equal to 5.

The movie bias was obvious and was to be introduced later in the Effects section.

### Users

Each movie rating was associated to a user. Users could rate one or several movies, and the following charts shows the number of ratings, for the ten most prolific raters, and the ratings of those users.

```{r Users - number of ratings, include=FALSE}
#User ratings analysis : 10 most raters
dataByUser <- edx %>% 
  group_by(userId) %>% 
  summarize(n=n()) %>%
  top_n(10,n)

#Add total ratings count to the top 10 raters
completeDataByUser <- left_join(edx %>% 
                             filter(userId %in% dataByUser$userId),
                           dataByUser, 
                           by="userId")

#Initialize the plot
userPlot <- ggplot()


userRatingsBarPlot <- userPlot  + 
  geom_bar(data=
             completeDataByUser %>% group_by(userId),
           aes(x = reorder(userId,-n),
               y = n),
           stat="identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  ylab("Number of ratings") + 
  xlab("Users (top 10 raters)") + 
  coord_cartesian(xlim =c(1, 10))

UserAvgRatingBoxplot <- userPlot + 
  geom_boxplot(data=completeDataByUser
    ,aes(x=reorder(userId,-n),
         y=rating)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  ylab("Ratings") + 
  xlab("Users (top 10 raters)") + 
  coord_cartesian(xlim =c(1, 10))

ggarrange(userRatingsBarPlot,UserAvgRatingBoxplot)

```

It was clear that the number of ratings did not affect the ratings by user. Some users tend to be more severe than others, and some tend to be give high ratings more easily. This induced a user bias that was described and used in the Effect section.

### Genre

Since we all tend to appreciate some genres more than others, it was logical to analyse the ratings depending of the genre. The following plots showed respectively the number of ratings versus the genre, and the repartition of the ratings by genre, for the 10 most reviewed genres.

```{r Genre, inclue=FALSE}

dataByGenre <- edx %>% 
  group_by(genres) %>% 
  summarize(n=n()) %>%
  top_n(10,n)

completeDataByGenre <- left_join(edx %>% 
                             filter(genres %in% dataByGenre$genres),
                           dataByGenre, 
                           by="genres")
genrePlot <- ggplot()

genreRatingsBarPlot <- genrePlot  + 
  geom_bar(data=dataByGenre,
           aes(x=reorder(genres,-n), 
               y=n),
           stat="identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  ylab("Number of ratings") + 
  xlab("Genres (top 10 rated)") + 
  coord_cartesian(xlim =c(1, 10))

UserAvgRatingBoxplot <- genrePlot + 
  geom_boxplot(
    data=completeDataByGenre,
    aes(x=reorder(genres,-n),
        y=rating)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  ylab("Ratings") + 
  xlab("Genres (top 10 rated)") + 
  coord_cartesian(xlim =c(1, 10))

ggarrange(genreRatingsBarPlot,UserAvgRatingBoxplot)

#genre effect
genreAvgs <- edx %>%
  group_by(genres) %>%
  summarize(b_g = mean(rating - mu_rating))

genreAvgs %>% qplot(b_g, geom ="histogram", bins = 10, data = ., color = I("black"))

Drama_DramaCrime = dataByGenre %>% arrange(desc(n)) %>% filter(genres %in% c("Drama","Crime|Drama"))

ratioDrama_DramaCrime = Drama_DramaCrime[1,]$n / Drama_DramaCrime[2,]$n

```

It appeared that there is no clear relationship between the number of ratings and the ratings. The most reviewed genre is Drama (25% of it's ratings between 4 and 4.5), but it seemed to be rated more harshly than the combination Crime/Drama (25% of it's ratings between 4 and 5). Knowing that Drama alone has been reviewed `r ratioDrama_DramaCrime` times more than Drama/Crime.

Comedy, which was the second most reviewed genre, very close to Drama, has a median rating of only 3, with a rather large interquartile range (25% between 3 and 4), but combined with at least another genre (Drama, or Drama and Romance for example), it's median rating went up (25% of it's rating are equal to 4 for the two said combination).

These observations led one to wonder about two biases : genre and the number of genre of each movie. These were to be detailled in the Effects section.

#### Primary genres

```{r Extracting primary genre, include=FALSE}
#Extracting primary genres
primarygenres <- edx %>% 
  filter(!str_detect(genres,"[|]")) %>% 
  pull(genres) %>% 
  unique()

genreDatas <- sapply(c(1:length(primarygenres)), function(index){
  dataByGenre = edx %>% 
    filter(str_detect(genres, primarygenres[index])) %>% 
    summarize(avg_rating=mean(rating), 
              nrating=n())
})

colnames(genreDatas) = primarygenres


```



### Year of release

### Date and time

## Executive summary 

section that describes the dataset and summarizes the goal of the project and key steps that were performed

## Methods/analysis section that explains the process and techniques used, such as data cleaning, data exploration and visualization, any insights gained, and your modeling approach



### Determining the model

Considering the rather large dataset at one's disposal, training algorithms as linear models cannot be used. One chose instead to fit the model :

$Y_{u,i} = \mu + \sum_{u,i} b_{n} + \varepsilon_{u,i}$

Where $Y$ is the rating prediction knowing all the parameters $u,i$, $\mu$ the average rating, and $b$ the effects for each parameter. $\varepsilon$ represents the error, or the residual. Since it was not possible to compute it, one was considering it as in the perfect scenario, which means $\varepsilon$ equal to zero.

The `b` were regularized using the penalized least squares, using a penalty term $\lambda$, which minimizes the residual sum of squares plus that penalty term, as:

$PLSE = \frac{1}{N} \sum_{u,i}(y_{u,i} - \mu - b_{i})^2 + \lambda \sum(b_{i})^2$

### Effects

#### Movie effect

```{r Movie effect, include=FALSE}
#average of ratings
mu_rating = mean(edx$rating)

#Movie effect
movieAvgs <- edx %>%
  group_by(movieId) %>%
  summarize(b_m = mean(rating - mu_rating))

movieAvgs %>% qplot(b_m, geom ="histogram", bins = 10, data = ., color = I("black"))

```

### Parameters

#### Creating training and test set

#### Picking penalty term

## Results

### Final model

###RMSE

## Conclusion

##Sources and references
